{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Sample Text\u001b[39;00m\n\u001b[0;32m     20\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI is transforming finance! Email: ai_research@finance.com, Phone: 9876543210. Born on 27-March-2000. Don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt ignore the impact of AI. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample Text\n",
    "text = \"AI is transforming finance! Email: ai_research@finance.com, Phone: 9876543210. Born on 27-March-2000. Don't ignore the impact of AI. \"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "unique_tokens = set(tokens)\n",
    "print(\"Unique Tokens Count:\", len(unique_tokens))\n",
    "\n",
    "# Count and Remove Punctuation\n",
    "punctuation_count = sum(1 for token in tokens if token in string.punctuation)\n",
    "text_no_punct = ''.join([char for char in text if char not in string.punctuation])\n",
    "print(\"Punctuation Count:\", punctuation_count)\n",
    "\n",
    "# Stopwords Distribution\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stopword_counts = Counter([word for word in tokens if word.lower() in stop_words])\n",
    "sns.barplot(x=list(stopword_counts.keys()), y=list(stopword_counts.values()))\n",
    "plt.title(\"Stopword Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Remove Stopwords\n",
    "filtered_text = ' '.join([word for word in tokens if word.lower() not in stop_words])\n",
    "print(\"Filtered Text:\", filtered_text)\n",
    "\n",
    "# POS Tagging Distribution\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_counts = Counter(tag for _, tag in pos_tags)\n",
    "sns.barplot(x=list(pos_counts.keys()), y=list(pos_counts.values()))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"POS Tag Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmas = set(lemmatizer.lemmatize(word.lower()) for word in tokens)\n",
    "print(\"Unique Lemma Count:\", len(lemmas))\n",
    "\n",
    "# Frequency Distribution of Top 10 Words\n",
    "word_freq = Counter(tokens)\n",
    "sns.barplot(x=[word for word, freq in word_freq.most_common(10)], y=[freq for _, freq in word_freq.most_common(10)])\n",
    "plt.title(\"Top 10 Word Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# N-grams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "quadgrams = list(ngrams(tokens, 4))\n",
    "print(\"Unique Bigrams:\", len(set(bigrams)))\n",
    "print(\"Unique Trigrams:\", len(set(trigrams)))\n",
    "print(\"Unique Quadgrams:\", len(set(quadgrams)))\n",
    "\n",
    "# Convert Dates to DD-MM-YYYY\n",
    "text = re.sub(r'\\b(\\d{1,2})[-](\\w+)[-](\\d{4})\\b', lambda x: f\"{x.group(1)}-{'03' if 'March' in x.group(2) else x.group(2)}-{x.group(3)}\", text)\n",
    "print(\"Formatted Dates:\", text)\n",
    "\n",
    "# Extract and Validate Phone Numbers\n",
    "valid_phone_pattern = re.compile(r'\\b\\d{10}\\b')\n",
    "phone_numbers = valid_phone_pattern.findall(text)\n",
    "print(\"Valid Phone Numbers:\", phone_numbers)\n",
    "\n",
    "# Year Distribution\n",
    "years = re.findall(r'\\b\\d{4}\\b', text)\n",
    "year_counts = Counter(years)\n",
    "sns.barplot(x=list(year_counts.keys()), y=list(year_counts.values()))\n",
    "plt.title(\"Year Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Insights & Applications\n",
    "insights = \"\"\"\n",
    "1. Stopwords impact text processing by increasing noise. Removing them enhances keyword relevance.\n",
    "2. POS tagging aids in NLP tasks like Named Entity Recognition and parsing.\n",
    "3. Lemmatization ensures words are reduced to meaningful roots for better text analysis.\n",
    "4. N-grams provide insights into commonly occurring phrases, useful for predictive text models.\n",
    "5. Identifying valid phone numbers and standardizing dates improve text consistency.\n",
    "\n",
    "Applications:\n",
    "1. Financial Market Analysis - NLP helps in sentiment analysis of news impacting stocks.\n",
    "2. Fraud Detection - Validating numbers and entity recognition assist in fraud prevention.\n",
    "3. Chatbots - Proper text processing ensures effective AI-driven conversations.\n",
    "4. Automated Report Generation - Structured text helps in summarizing financial reports.\n",
    "\"\"\"\n",
    "print(insights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
